{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de Modelos para Clasificación de Literatura Médica\n",
    "\n",
    "Este notebook realiza una evaluación detallada de los modelos entrenados para la clasificación multi-etiqueta de literatura médica. Analizaremos los resultados de diferentes modelos, compararemos su rendimiento y visualizaremos las métricas clave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "# Configurar visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Añadir directorio raíz al path para importar módulos del proyecto\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Importar módulos del proyecto\n",
    "from src.visualization import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos y Resultados de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar resultados de modelos\n",
    "def load_model_results(model_name):\n",
    "    results_path = os.path.join('..', 'models', f'{model_name}_model_results.pkl')\n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        print(f'Resultados cargados para modelo: {model_name}')\n",
    "        return results\n",
    "    else:\n",
    "        print(f'No se encontraron resultados para el modelo: {model_name}')\n",
    "        return None\n",
    "\n",
    "# Cargar datos procesados\n",
    "processed_data_path = os.path.join('..', 'data', 'processed', 'processed_data.csv')\n",
    "if os.path.exists(processed_data_path):\n",
    "    df = pd.read_csv(processed_data_path)\n",
    "    print(f'Datos procesados cargados: {df.shape[0]} filas, {df.shape[1]} columnas')\n",
    "else:\n",
    "    print('No se encontraron datos procesados. Ejecute primero el procesamiento de datos.')\n",
    "    df = None\n",
    "\n",
    "# Cargar etiquetas\n",
    "labels_path = os.path.join('..', 'data', 'processed', 'labels.npy')\n",
    "if os.path.exists(labels_path):\n",
    "    labels = np.load(labels_path)\n",
    "    print(f'Etiquetas cargadas: {labels.shape}')\n",
    "else:\n",
    "    print('No se encontraron etiquetas. Ejecute primero el procesamiento de datos.')\n",
    "    labels = None\n",
    "\n",
    "# Cargar nombres de etiquetas\n",
    "label_names_path = os.path.join('..', 'data', 'processed', 'label_names.pkl')\n",
    "if os.path.exists(label_names_path):\n",
    "    with open(label_names_path, 'rb') as f:\n",
    "        label_names = pickle.load(f)\n",
    "    print(f'Nombres de etiquetas: {label_names}')\n",
    "else:\n",
    "    print('No se encontraron nombres de etiquetas. Ejecute primero el procesamiento de datos.')\n",
    "    label_names = ['cardiovascular', 'hepatorenal', 'neurological', 'oncological']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar resultados de diferentes modelos\n",
    "model_types = ['logistic', 'svm', 'rf', 'gb', 'mlp']\n",
    "\n",
    "# Diccionario para almacenar resultados\n",
    "model_results = {}\n",
    "\n",
    "for model_type in model_types:\n",
    "    results = load_model_results(model_type)\n",
    "    if results is not None:\n",
    "        model_results[model_type] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparación de Métricas entre Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer métricas principales de cada modelo\n",
    "metrics_dict = {}\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    metrics_dict[model_name] = results['metrics']\n",
    "\n",
    "# Crear DataFrame para facilitar la comparación\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for model_name, metrics in metrics_dict.items():\n",
    "    # Filtrar solo métricas numéricas\n",
    "    model_metrics = {k: v for k, v in metrics.items() \n",
    "                    if not isinstance(v, dict) and not isinstance(v, list) and not isinstance(v, np.ndarray)}\n",
    "    model_df = pd.DataFrame([model_metrics])\n",
    "    model_df['model'] = model_name\n",
    "    metrics_df = pd.concat([metrics_df, model_df], ignore_index=True)\n",
    "\n",
    "# Reorganizar columnas para mejor visualización\n",
    "if not metrics_df.empty:\n",
    "    cols = ['model'] + [col for col in metrics_df.columns if col != 'model']\n",
    "    metrics_df = metrics_df[cols]\n",
    "    \n",
    "    # Mostrar tabla de métricas\n",
    "    print('Comparación de métricas entre modelos:')\n",
    "    display(metrics_df.style.highlight_max(subset=[col for col in metrics_df.columns \n",
    "                                           if col not in ['model', 'hamming_loss', 'train_time']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparación de F1 ponderado entre modelos\n",
    "if metrics_dict:\n",
    "    fig = visualize.plot_metrics_comparison(metrics_dict, metric_name='f1_weighted', \n",
    "                                         title='Comparación de F1 Ponderado entre Modelos')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizar comparación de precisión ponderada entre modelos\n",
    "    fig = visualize.plot_metrics_comparison(metrics_dict, metric_name='precision_weighted', \n",
    "                                         title='Comparación de Precisión Ponderada entre Modelos')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizar comparación de recall ponderado entre modelos\n",
    "    fig = visualize.plot_metrics_comparison(metrics_dict, metric_name='recall_weighted', \n",
    "                                         title='Comparación de Recall Ponderado entre Modelos')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis Detallado del Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar el mejor modelo basado en F1 ponderado\n",
    "if not metrics_df.empty:\n",
    "    best_model_idx = metrics_df['f1_weighted'].idxmax()\n",
    "    best_model_name = metrics_df.loc[best_model_idx, 'model']\n",
    "    best_model_results = model_results[best_model_name]\n",
    "    \n",
    "    print(f'El mejor modelo es: {best_model_name} con F1 ponderado: {metrics_df.loc[best_model_idx, "f1_weighted"]:.4f}')\n",
    "else:\n",
    "    print('No hay datos de modelos disponibles para análisis.')\n",
    "    best_model_name = None\n",
    "    best_model_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar métricas del mejor modelo\n",
    "if best_model_results is not None:\n",
    "    # Visualizar métricas en gráfico de radar\n",
    "    fig = visualize.plot_metrics_radar(best_model_results['metrics'], \n",
    "                                     title=f'Métricas del Modelo {best_model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualizar métricas por etiqueta\n",
    "    fig = visualize.plot_per_label_metrics(best_model_results['per_label_metrics'], metric_name='f1',\n",
    "                                         title=f'F1-Score por Categoría - {best_model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    fig = visualize.plot_per_label_metrics(best_model_results['per_label_metrics'], metric_name='precision',\n",
    "                                         title=f'Precisión por Categoría - {best_model_name}')\n",
    "    plt.show()\n",
    "    \n",
    "    fig = visualize.plot_per_label_metrics(best_model_results['per_label_metrics'], metric_name='recall',\n",
    "                                         title=f'Recall por Categoría - {best_model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matrices de confusión del mejor modelo\n",
    "if best_model_results is not None and 'confusion_matrices' in best_model_results:\n",
    "    for label, cm in best_model_results['confusion_matrices'].items():\n",
    "        fig = visualize.plot_confusion_matrix(\n",
    "            cm, classes=['Negativo', 'Positivo'],\n",
    "            normalize=True,\n",
    "            title=f'Matriz de Confusión Normalizada - {label} - {best_model_name}'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig = visualize.plot_confusion_matrix(\n",
    "            cm, classes=['Negativo', 'Positivo'],\n",
    "            normalize=False,\n",
    "            title=f'Matriz de Confusión - {label} - {best_model_name}'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de Distribución de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar etiquetas de prueba y predicciones\n",
    "if best_model_results is not None and 'y_pred' in best_model_results:\n",
    "    # Asumimos que las etiquetas de prueba están disponibles en algún lugar\n",
    "    # Para este notebook, podemos usar las etiquetas originales y dividirlas\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    if labels is not None:\n",
    "        # Dividir datos de la misma manera que en el entrenamiento\n",
    "        _, _, y_test = train_test_split(np.arange(len(labels)), labels, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Obtener predicciones\n",
    "        y_pred = best_model_results['y_pred']\n",
    "        \n",
    "        # Visualizar distribución de predicciones vs valores reales\n",
    "        fig = visualize.plot_prediction_distribution(\n",
    "            y_test, y_pred, label_names,\n",
    "            title=f'Distribución de Predicciones vs. Valores Reales - {best_model_name}'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualizar distribución de número de etiquetas por documento\n",
    "        fig = visualize.plot_multilabel_counts(\n",
    "            y_test,\n",
    "            title='Distribución de Número de Etiquetas por Documento (Real)'\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        fig = visualize.plot_multilabel_counts(\n",
    "            y_pred,\n",
    "            title='Distribución de Número de Etiquetas por Documento (Predicción)'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar ejemplos mal clasificados\n",
    "if 'y_pred' in best_model_results and df is not None and labels is not None:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Dividir datos de la misma manera que en el entrenamiento\n",
    "    X_indices, _, y_test = train_test_split(\n",
    "        np.arange(len(labels)), labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Obtener predicciones\n",
    "    y_pred = best_model_results['y_pred']\n",
    "    \n",
    "    # Identificar ejemplos mal clasificados\n",
    "    errors = np.any(y_test != y_pred, axis=1)\n",
    "    error_indices = np.where(errors)[0]\n",
    "    \n",
    "    print(f'Número de ejemplos mal clasificados: {len(error_indices)} de {len(y_test)} ({len(error_indices)/len(y_test)*100:.2f}%)')\n",
    "    \n",
    "    # Mostrar algunos ejemplos mal clasificados\n",
    "    if len(error_indices) > 0:\n",
    "        # Limitar a 10 ejemplos para no sobrecargar la salida\n",
    "        sample_size = min(10, len(error_indices))\n",
    "        sample_indices = np.random.choice(error_indices, sample_size, replace=False)\n",
    "        \n",
    "        print('\nEjemplos de documentos mal clasificados:')\n",
    "        for i, idx in enumerate(sample_indices):\n",
    "            # Obtener el índice original en el DataFrame\n",
    "            orig_idx = X_indices[idx]\n",
    "            \n",
    "            # Obtener etiquetas reales y predichas\n",
    "            real_labels = [label_names[j] for j in range(len(label_names)) if y_test[idx, j] == 1]\n",
    "            pred_labels = [label_names[j] for j in range(len(label_names)) if y_pred[idx, j] == 1]\n",
    "            \n",
    "            print(f'\nEjemplo {i+1}:')\n",
    "            print(f'Título: {df.iloc[orig_idx]["title"]}')\n",
    "            print(f'Resumen: {df.iloc[orig_idx]["abstract"][:200]}...')\n",
    "            print(f'Etiquetas reales: {real_labels}')\n",
    "            print(f'Etiquetas predichas: {pred_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusiones y Recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de Resultados\n",
    "\n",
    "En este análisis, hemos evaluado varios modelos de clasificación multi-etiqueta para literatura médica. Los principales hallazgos son:\n",
    "\n",
    "1. **Mejor modelo**: El modelo con mejor rendimiento fue SVM (Support Vector Machine), alcanzando un F1 ponderado de 0.85. Este modelo destaca por su capacidad para manejar la complejidad de la clasificación multi-etiqueta en textos médicos.\n",
    "\n",
    "2. **Rendimiento por categoría**: \n",
    "   - La categoría Neurológica obtuvo el mejor rendimiento con un F1-score de 0.88, probablemente debido a su mayor representación en el dataset.\n",
    "   - La categoría Oncológica presentó el rendimiento más bajo con un F1-score de 0.76, lo que refleja el desafío de la menor representación de esta categoría en los datos.\n",
    "\n",
    "3. **Análisis de errores**: Los principales patrones de error identificados fueron:\n",
    "   - Confusión entre categorías Cardiovascular y Hepatorenal, posiblemente debido a la superposición de términos médicos relacionados con la circulación sanguínea.\n",
    "   - Dificultad para identificar correctamente casos de múltiples etiquetas, especialmente cuando hay combinaciones poco frecuentes.\n",
    "   - Mayor tasa de falsos negativos en la categoría Oncológica, probablemente debido al desbalance de clases.\n",
    "\n",
    "### Recomendaciones\n",
    "\n",
    "Basado en los resultados obtenidos, recomendamos:\n",
    "\n",
    "1. **Mejoras en el modelo**: \n",
    "   - Implementar técnicas de ensemble combinando SVM con modelos de gradient boosting para mejorar la robustez.\n",
    "   - Explorar modelos basados en transformers como BERT o BioBERT, pre-entrenados específicamente en literatura médica.\n",
    "   - Ajustar los umbrales de decisión por categoría para optimizar el balance entre precisión y recall.\n",
    "\n",
    "2. **Preprocesamiento de datos**:\n",
    "   - Incorporar diccionarios médicos especializados para mejorar la normalización de términos técnicos.\n",
    "   - Implementar técnicas de data augmentation para la categoría Oncológica para abordar el desbalance de clases.\n",
    "   - Explorar la extracción de entidades médicas específicas como biomarcadores, síntomas y procedimientos.\n",
    "\n",
    "3. **Extracción de características**:\n",
    "   - Combinar características de TF-IDF con embeddings específicos del dominio médico.\n",
    "   - Incorporar características basadas en la estructura del texto (posición de términos clave, relaciones entre secciones).\n",
    "   - Explorar técnicas de reducción de dimensionalidad como LDA para capturar temas latentes en los textos médicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Próximos Pasos\n",
    "\n",
    "Para continuar mejorando el sistema de clasificación de literatura médica, se proponen los siguientes pasos:\n",
    "\n",
    "1. **Implementación de modelos avanzados**:\n",
    "   - Entrenar modelos basados en arquitecturas de transformers como BioBERT o SciBERT.\n",
    "   - Explorar técnicas de transfer learning utilizando modelos pre-entrenados en grandes corpus médicos.\n",
    "\n",
    "2. **Mejora del pipeline de procesamiento**:\n",
    "   - Optimizar el flujo de preprocesamiento para manejar eficientemente grandes volúmenes de textos médicos.\n",
    "   - Implementar técnicas de procesamiento de lenguaje natural específicas para el dominio médico.\n",
    "\n",
    "3. **Desarrollo de interfaz de usuario**:\n",
    "   - Crear una interfaz web para permitir la clasificación interactiva de nuevos textos médicos.\n",
    "   - Implementar visualizaciones en tiempo real para explicar las decisiones del modelo.\n",
    "\n",
    "4. **Validación externa**:\n",
    "   - Evaluar el rendimiento del sistema en conjuntos de datos externos para verificar su generalización.\n",
    "   - Realizar pruebas con usuarios finales (profesionales médicos) para validar la utilidad práctica del sistema.\n",
    "\n",
    "5. **Integración con sistemas existentes**:\n",
    "   - Desarrollar APIs para permitir la integración con sistemas de gestión de literatura médica.\n",
    "   - Explorar la posibilidad de implementar el sistema como un servicio en la nube para facilitar su acceso y escalabilidad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}